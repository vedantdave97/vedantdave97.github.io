<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Vedant Dave</title>
    
    <meta name="author" content="Jon Barron">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <!-- Add Font Awesome -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">

  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Vedant Dave
                </p>
                <p> Hey! I'm a Ph.D. student at <a href="https://cps.unileoben.ac.at/">Cyber-Physical-Systems Lab</a> at <a href="https://www.unileoben.ac.at/">Montanuniversität Leoben</a> in Austria and advised by <a href="https://cps.unileoben.ac.at/prof-elmar-rueckert/">Elmar Rueckert</a>. I'm broadly interested in finding better representations from multiple sensory modalities under noisy conditions. My work involves unsupervised RL, robust representation in RL, Multimodality, and Robot Manipulation.
                </p>
                <p>
                  I received my Master's degree in Automation and Robotics from <a href="https://www.tu-dortmund.de/">Technische Universität Dortmund</a> in 2021, focusing on Robotics and Artificial Intelligence. My thesis, entitled “Model-agnostic Reinforcement Learning Solution for Autonomous Programming of Robotic Motion,” was completed at <a href="https://www.mercedes-benz.com/en/">Mercedes-Benz AG</a>, where I implemented reinforcement learning for motion planning of manipulators in complex environments. Prior to this, I was fortunate to work as a research intern with <a href="https://scholar.google.com/citations?user=vLWgi-YAAAAJ&hl=en">Leonel Rozo</a> at the <a href="https://www.bosch-ai.com/">Bosch Center for Artificial Intelligence</a>, where I worked on <a href="https://proceedings.mlr.press/v164/rozo22a.html">Probabilistic Movement Primitives on Riemannian Manifolds</a>.
                </p>
                <p style="text-align:center">
                  <a href="mailto:vedant.dave@unileoben.ac.at" title="Email"><i class="fas fa-envelope"></i></a> &nbsp;/&nbsp;
                      <a href="data/JonBarron-CV.pdf" title="CV"><i class="fas fa-file-alt"></i></a> &nbsp;/&nbsp;
                      <a href="https://scholar.google.com/citations?hl=en&user=8Gi6AaEAAAAJ" title="Google Scholar"><i class="fas fa-graduation-cap"></i></a> &nbsp;/&nbsp;
                      <a href="https://www.semanticscholar.org/author/Vedant-Dave/2138217785" title="Semantic Scholar"><i class="fas fa-brain"></i></a> &nbsp;/&nbsp;
                      <a href="https://de.linkedin.com/in/vedant-dave-095629178" title="LinkedIn"><i class="fab fa-linkedin"></i></a> &nbsp;/&nbsp;
                      <a href="https://github.com/vedantdave97" title="GitHub"><i class="fab fa-github"></i></a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:25%">
                <a href="images/VedantDave.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/VedantDave.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2> Publications and Preprints</h2>  (* indicates equal contribution)
            </td>
          </tr>
        </tbody></table>
      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="dpi_stop()" onmouseover="dpi_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dpi_image'><img src='images/dpi.gif' width=100% height=100%></div>
                <img src='images/dpi.jpg' width=100% height=100%>
              </div>
              <script type="text/javascript">
                function dpi_start() {
                  document.getElementById('dpi_image').style.opacity = "1";
                }
      
                function dpi_stop() {
                  document.getElementById('dpi_image').style.opacity = "0";
                }
                dpi_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/pdf?id=MdHDUsP2lt">
                <span class="papertitle">Information-Theoretic World Model learning for Denoised Predictions</span>
              </a>
              <br>
          <strong>Vedant Dave</strong>,
          <a href="https://cps.unileoben.ac.at/prof-elmar-rueckert/">Elmar Rueckert</a>
              <br>
              <em>arxiv</em> 2024
              <br>
              <a href="https://openreview.net/pdf?id=MdHDUsP2lt">paper</a>
              <p>
              DPI is an information-theoretic approach for reinforcement learning that minimizes past information while retaining future-relevant data, enabling denoised predictions. Using Soft Actor-Critic agents with an auxiliary loss, the method outperforms nine state-of-the-art approaches in complex environments with natural video backgrounds.
              </p>
            </td>
          </tr>
        

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr onmouseout="m2curl_stop()" onmouseover="m2curl_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='m2curl_image'><img src='images/m2curl.gif' width=100% height=100%></div>
          <img src='images/m2curl.jpg' width=100% height=75%>
        </div>
        <script type="text/javascript">
          function m2curl_start() {
            document.getElementById('m2curl_image').style.opacity = "1";
          }

          function m2curl_stop() {
            document.getElementById('m2curl_image').style.opacity = "0";
          }
          m2curl_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2401.17032">
          <span class="papertitle">M2CURL: Sample-Efficient Multimodal Reinforcement Learning via Self-Supervised Representation Learning for Robotic Manipulation</span>
        </a>
        <br>
		<a href="https://cps.unileoben.ac.at/fotios-lygerakis-m-sc/">Fotios Lygerakis</a>,
		<strong>Vedant Dave</strong>,
		<a href="https://cps.unileoben.ac.at/prof-elmar-rueckert/">Elmar Rueckert</a>
        <br>
        <em>IEEE International Conference on Ubiquitous Robots (UR)</em> 2024
        <br>
        <em>ProxyTouch Workshop, ICRA</em> 2024
        <br>
        <a href="https://sites.google.com/view/M2CURL/home">project page</a>
        /
        <a href="https://arxiv.org/abs/2401.17032">paper</a>
        <p></p>
        <p>
          M2CURL builds on MViTac to improves RL by efficiently integrating visual and tactile representations. It accelerates learning in downstream manipulation tasks.
        </p>
      </td>
    </tr>
	

    <tr onmouseout="mvitac_stop()" onmouseover="mvitac_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/mvitac.jpg' width=100% height=90%>
        </div>
        <script type="text/javascript">
          function mvitac_start() {
            document.getElementById('nuvo_image').style.opacity = "1";
          }

          function mvitac_stop() {
            document.getElementById('nuvo_image').style.opacity = "0";
          }
          mvitac_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2401.12024">
          <span class="papertitle">Multimodal Visual-Tactile Representation Learning through Self-Supervised Contrastive Pre-Training</span>
        </a>
        <br>
        <strong>Vedant Dave*</strong>,
        <a href="https://cps.unileoben.ac.at/fotios-lygerakis-m-sc/">Fotios Lygerakis*</a>,
        <a href="https://cps.unileoben.ac.at/prof-elmar-rueckert/">Elmar Rueckert</a>
        <br>
        <em>IEEE International Conference on Robotics and Automation (ICRA)</em> 2024
        <br>
        <a href="https://sites.google.com/view/mvitac/home">project page</a>
        /
        <a href="https://github.com/ligerfotis/mvitac">code</a>
        /
        <a href="https://arxiv.org/abs/2401.12024">paper</a>
        /
        <a href="https://www.youtube.com/watch?v=Z7b-QOP1CTo">video</a>
        <p></p>
        <p>
          MViTac is self-supervised approach that integrates vision and tactile modalities using contrastive learning. MViTac utilizes intra- and inter-modality relationships to learn a shared representation space. Experiments show that MViTac outperforms state-of-the-art methods in downstream tasks, which are performed by linear probing.
        </p>
      </td>
    </tr>
	
	
    <tr onmouseout="tac2_stop()" onmouseover="tac2_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='tacpromps2_image'><img src='images/tacpromps2.gif' width=100% height=100%></div>
          <img src='images/tacpromps2.jpg' width=100% height=75%>
        </div>
        <script type="text/javascript">
          function tac2_start() {
            document.getElementById('tacpromps2_image').style.opacity = "1";
          }

          function tac2_stop() {
            document.getElementById('tacpromps2_image').style.opacity = "0";
          }
          tac2_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://drive.google.com/file/d/18BSlOC-HrYdrfAL3RiHjF7a6RNmDrgDo/view">
			<span class="papertitle">Can we infer the full-arm manipulation skills from tactile targets?</span>
        </a>
        <br>
		    <strong>Vedant Dave</strong>,
        <a href="https://cps.unileoben.ac.at/prof-elmar-rueckert/">Elmar Rueckert</a>
        <br>
        <em>Workshop on Advances in Close Proximity Human-Robot Collaboration, Humanoids</em> 2022
        <br>
        <a href="https://drive.google.com/file/d/18BSlOC-HrYdrfAL3RiHjF7a6RNmDrgDo/view">paper</a>
        <p>
          This paper contains late-breaking results for TacProMPs, which learns and predicts complex arm movements based on tactile responses. Experiments were conducted on the real robot with a wide variety of objects.
        </p>
      </td>
    </tr>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr onmouseout="tacpromps_stop()" onmouseover="tacpromps_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='tacpromps_image'><img src='images/tacpromps.gif' width=100% height=100%></div>
          <img src='images/tacpromps.jpg' width=100% height=75%>
        </div>
        <script type="text/javascript">
          function tacpromps_start() {
            document.getElementById('tacpromps_image').style.opacity = "1";
          }

          function tacpromps_stop() {
            document.getElementById('tacpromps_image').style.opacity = "0";
          }
          tacpromps_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/abstract/document/9999743/">
			<span class="papertitle">Predicting full-arm grasping motions from anticipated tactile responses</span>
        </a>
        <br>
		    <strong>Vedant Dave</strong>,
        <a href="https://cps.unileoben.ac.at/prof-elmar-rueckert/">Elmar Rueckert</a>
        <br>
        <em>IEEE-RAS International Conference on Humanoid Robots (Humanoids)</em> 2022 <strong>(Oral Presentation)</strong>
        <br>
        <a href="https://ieeexplore.ieee.org/abstract/document/9999743/">paper</a>
        /
        <a href="https://www.youtube.com/watch?v=Z7b-QOP1CTo">video</a>
        <p></p>
        <p>
        TacProMPs learns and predicts complex arm movements based on tactile responses, particularly for manipulating non-uniform objects, demonstrating adaptability diverse grasping scenarios.</p>
      </td>
    </tr>


<tr onmouseout="ori_stop()" onmouseover="ori_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
      <div class="two" id='oripromps_image'><img src='images/oripromps.gif' width=100% height=100%></div>
      <img src='images/oripromps.jpg' width=100%>
    </div>
    <script type="text/javascript">
      function ori_start() {
        document.getElementById('oripromps_image').style.opacity = "1";
      }

      function ori_stop() {
        document.getElementById('oripromps_image').style.opacity = "0";
      }
      ori_stop()
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://proceedings.mlr.press/v164/rozo22a.html">
      <span class="papertitle">Orientation Probabilistic Movement Primitives on Riemannian Manifolds</span>
    </a>
    <br>
    <a href="https://scholar.google.com/citations?user=vLWgi-YAAAAJ&hl=en">Leonel Rozo*</a>,
    <strong>Vedant Dave*</strong>
    <br>
    <em>Conference on Robot Learning (CoRL)</em> 2022
    <br>
    <a href="https://sites.google.com/view/orientation-promp">project page</a>
    /
    <a href="https://proceedings.mlr.press/v164/rozo22a.html">paper</a>
    <p>
      This paper introduces a Riemannian formulation of ProMPs for encoding and retrieving quaternion trajectories, enabling full-pose robot motions in operational spaces. This method builds on Riemannian manifold theory and exploits multilinear geodesic regression for estimating the ProMPs parameters.        
    </p>
  </td>
</tr>          


      </tbody></table>
          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Industrial Collaborations</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/marienhuette.jpg" width=100% height=10% style="margin-top: -20px;"></td>
              <td width="75%" valign="center">
                <a href="https://www.marienhuette.at/"> Stahl- und Walzwerk Marienhütte GmbH
                </a>
                <p>
                  <strong>Consultant</strong> <br> 
                  2022/03-2023/02
                </p>
                <p> 
                  Predicting Yield Strength of different materials from production process. Designing Neural Networks and optimization. Found out error in measurement inaccuracy from data analysis.                  
                </p>
              </td>
            </tr>
            
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
              <tr>
                <td>
                  <h2>Academic Collaborations</h2>
                </td>
              </tr>
            </tbody></table>
            <table width="100%" align="center" border="0" cellpadding="20"><tbody>
              <tr>
                <td style="padding:0px;width:25%;vertical-align:middle"><img src="images/dge_logo.jpg" width=100% height=10% style="margin-top: 0px;"></td>
                <td width="75%" valign="center">
                  <a href="https://pureadmin.unileoben.ac.at/portal/de/publications/a-reinforcement-learning-approach-for-realtime-autonomous-decisionmaking-in-well-construction(9cd6800f-71a6-401f-95d2-f2b529c7f437).html">A Reinforcement Learning Approach for Decision-Making in Wells</a><br>
                  <span style="color:red;">Paper Upcoming!</span>
                  <p> 
                    This research evaluates hole conditioning operations in wellbore drilling, focusing on activities like circulation, reaming, and washing to ensure well integrity. An agent is trained with model-free RL coming from the online data from the real-world scenarios.
                  </p>
                </td>
              </tr>

              <table width="100%" align="center" border="0" cellpadding="20"><tbody>
                <tr>
                  <td style="padding:0px;width:25%;vertical-align:middle"><img src="images/aac_symbol.jpg" width=100% height=10% style="margin-top: 0px;"></td>
                  <td width="75%" valign="center">
                    <a href="https://pure.unileoben.ac.at/de/publications/green-and-blue-infrastructure-as-model-system-for-emissions-of-te">Green Facade</a><br>
                    <span style="color:red;">Paper Upcoming!</span>
                    <p>
                      <strong>Coding and Supervision</strong> <br> 
                    </p>
                    <p> 
                      This study predicts the floor level from the distribution of TCEs (Li, Be, V, Ga, Ge, Nb, Sb, Te, Ta, Tl, Bi, and REYs) in Vienna's urban aerosol.                      
                    </p>
                  </td>
                </tr>

                <table width="100%" align="center" border="0" cellpadding="20"><tbody>
                  <tr>
                    <td style="padding:0px;width:25%;vertical-align:middle"><img src="images/material_science_group.jpg" width=100% height=10% style="margin-top: 0px;"></td>
                    <td width="75%" valign="center">
                      <a href="https://pure.unileoben.ac.at/en/publications/physics-informed-neural-network-for-predicting-gibbs-free-energy">Physics-informed neural network for predicting Gibbs free energy</a><br>
                      <span style="color:red;">Paper Upcoming!</span>
                      <p>
                        <strong>Coding and Supervision</strong> <br> 
                      </p>
                      <p> 
                        A physics-informed neural network combined with the CALPHAD formalism predicts Gibbs energy in alloys by determining the Redlich-Kister parameter using novel descriptors. This method enhances CALPHAD parameterization, expediting materials development and phase stability determination with high accuracy and potential.                  
                        </p>
                    </td>
                  </tr>

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
              <tr>
                <td>
                  <h2>Teaching Experience</h2>
                </td>
              </tr>
            </tbody></table>
            <table width="100%" align="center" border="0" cellpadding="20"><tbody>
              <tr>
              <td style="padding:5px;width:25%;vertical-align:middle">
                <img src="images/muleoben.jpg" width=50% height=5% alt="cs188" style="margin-left: 50px;">
              </td>
              <td width="75%" valign="center">
                <a href="https://cps.unileoben.ac.at/150-013-exercises-in-machine-learning-2sh-p-ss-2021-22/">Tutorial, Introduction to Machine Learning Lab SS 2024</a>
                <br>
                <a href="https://online.unileoben.ac.at/mu_online/ee/ui/ca2/app/desktop/#/slc.tm.cp/student/courses/3217238?$ctx=design=ca;lang=de&$scrollTo=toc_overview">Teaching Assistant, Introduction to Python WS 2023</a>
                <br>
                <a href="https://cps.unileoben.ac.at/190-002-cyber-physical-systems-lab-2sh-p-ws-2022-23/">Teaching Assistant, Cyber-Physical Systems Lab WS 2022</a>
              </td>
            </tr>
            
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
              <tr>
                <td>
                  <h2>Reviewing Services</h2>
                </td>
              </tr>
            </tbody></table>
            <table width="100%" align="center" border="0" cellpadding="20"><tbody>
              <tr>
                <td style="padding:0px;width:25%;vertical-align:middle">
                  <img src="images/neurips.jpg" alt="cs188" width=75% height=10%  style="margin-left:45px; margin-top:-25px;">
                </td>
                <td width="75%" valign="middle" style="padding-top: 2px;">
                  <p style="margin-top: -0px;">
                    <strong>2024</strong>: NeurIPS, ECAI, CoRL, IROS, BioRob
                    <br>
                    <strong>2023</strong>: ECAI, CoRL, IROS, RAL
                    <br>
                    <strong>2022</strong>: ECAI, CoRL, IROS, RAL, Humanoids  
                  </p>
                </td>
              </tr>
            
              <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                  <tr>
                    <td>
                      <h2>Media</h2>
                    </td>
                  </tr>
                </tbody>
              </table>
              <table width="50%" align="center" border="0" cellpadding="20">
                <tbody>
                  <tr>
                    <td style="padding:0px;width:25%;vertical-align:middle">
                      <a href="https://www.rundschau-medien.at/epaper/ausgaben/2024/2420/677492/LE/16/" target="_blank">
                        <img src="images/osr_newspaper_page.png" alt="osr" width="75%" height="10%" style="margin-left:0px; margin-top:-10px;">
                      </a>
                    </td>
                    <td style="padding:0px;width:25%;vertical-align:middle">
                      <a href="https://commulity.unileoben.ac.at/detail/a-life-of-their-own-autonomous-robotics" target="_blank">
                        <img src="images/commulity.jpg" alt="commulity" width="75%" height="50%" style="margin-left:45px; margin-top:-50px;">
                      </a>
                    </td>
                  </tr>
              </table>
              

          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  <a href="https://github.com/jonbarron/jonbarron_website">Template</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
